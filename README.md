# ğŸ³ LlamaBase

A simple Docker setup for running Ollama locally with a web interface. Run AI models on your own machine, with your data staying private and secure.

## ğŸš€ Quick Start

```bash
# Clone and enter the repository
git clone git@github.com:aze3ma/LlamaBase.git
cd LlamaBase

# Start everything with one command
./start.sh
```

That's it! The script will set up everything you need.

## ğŸ“‹ Requirements

-   Docker
-   Docker Compose
-   8GB RAM (16GB recommended)
-   20GB disk space

## ğŸ¯ What You Get

-   Web interface at http://localhost:8080
-   Command-line access via `ollama` command
-   Your choice of AI models (llama2, mistral, etc.)
-   All data stored locally on your machine

## ğŸ’» Basic Usage

### Web Interface

1. Open http://localhost:8080
2. Create an account
3. Start chatting!

### Command Line

```bash
# List your models
ollama list

# Chat with a model
ollama run mistral
```

## ğŸ”§ Troubleshooting

If something goes wrong:

1. Check Docker logs: `docker logs ollama`
2. Restart services: `docker-compose restart`
3. Make sure ports 11434 and 8080 are free

## ğŸ“ License

MIT License

---

Made with â¤ï¸ by aze3ma
